{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Pyspark and Creating Spark Session:"
      ],
      "metadata": {
        "id": "9TseLiI008-R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjCXpbUyvvOy",
        "outputId": "880eefd7-6904-46d4-a81c-4b2d1cace87e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "# Install PySpark\n",
        "!pip install pyspark\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BookRecommendationSystem\") \\\n",
        "    .getOrCreate()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the Big Data sets:"
      ],
      "metadata": {
        "id": "YC3if7oN1IZh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Book Name and ID with their Average Ratings:"
      ],
      "metadata": {
        "id": "8HhAcjDr1_kG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the columns you want to select\n",
        "selected_columns = [\"Id\", \"Name\"]\n",
        "\n",
        "# Define the file names url from the google drive\n",
        "file_names = [\n",
        "    \"/content/drive/MyDrive/Data Science Projects/Book Recommendation System/Datasets/book1-100k.csv\",\n",
        "    \"/content/drive/MyDrive/Data Science Projects/Book Recommendation System/Datasets/book1000k-1100k.csv\",\n",
        "    \"/content/drive/MyDrive/Data Science Projects/Book Recommendation System/Datasets/book100k-200k.csv\",\n",
        "    '/content/drive/MyDrive/Data Science Projects/Book Recommendation System/Datasets/book1100k-1200k.csv',\n",
        "    '/content/drive/MyDrive/Data Science Projects/Book Recommendation System/Datasets/book1200k-1300k.csv',\n",
        "    '/content/drive/MyDrive/Data Science Projects/Book Recommendation System/Datasets/book1300k-1400k.csv',\n",
        "    '/content/drive/MyDrive/Data Science Projects/Book Recommendation System/Datasets/book1400k-1500k.csv',\n",
        "    '/content/drive/MyDrive/Data Science Projects/Book Recommendation System/Datasets/book1500k-1600k.csv',\n",
        "    '/content/drive/MyDrive/Data Science Projects/Book Recommendation System/Datasets/book1600k-1700k.csv',\n",
        "    '/content/drive/MyDrive/Data Science Projects/Book Recommendation System/Datasets/book1800k-1900k.csv',\n",
        "    '/content/drive/MyDrive/Data Science Projects/Book Recommendation System/Datasets/book1700k-1800k.csv',\n",
        "    '/content/drive/MyDrive/Data Science Projects/Book Recommendation System/Datasets/book1900k-2000k.csv',\n",
        "    '/content/drive/MyDrive/Data Science Projects/Book Recommendation System/Datasets/book2000k-3000k.csv',\n",
        "    '/content/drive/MyDrive/Data Science Projects/Book Recommendation System/Datasets/book200k-300k.csv',\n",
        "    '/content/drive/MyDrive/Data Science Projects/Book Recommendation System/Datasets/book3000k-4000k.csv',\n",
        "    '/content/drive/MyDrive/Data Science Projects/Book Recommendation System/Datasets/book300k-400k.csv',\n",
        "    '/content/drive/MyDrive/Data Science Projects/Book Recommendation System/Datasets/book4000k-5000k.csv',\n",
        "    '/content/drive/MyDrive/Data Science Projects/Book Recommendation System/Datasets/book400k-500k.csv',\n",
        "    '/content/drive/MyDrive/Data Science Projects/Book Recommendation System/Datasets/book500k-600k.csv',\n",
        "    '/content/drive/MyDrive/Data Science Projects/Book Recommendation System/Datasets/book600k-700k.csv',\n",
        "    '/content/drive/MyDrive/Data Science Projects/Book Recommendation System/Datasets/book700k-800k.csv',\n",
        "    '/content/drive/MyDrive/Data Science Projects/Book Recommendation System/Datasets/book800k-900k.csv',\n",
        "    '/content/drive/MyDrive/Data Science Projects/Book Recommendation System/Datasets/book900k-1000k.csv',\n",
        "    # Add other file names url  here\n",
        "\n",
        "]\n",
        "\n",
        "# Load each CSV file into separate DataFrames\n",
        "dataframes = {}\n",
        "for i, file_name in enumerate(file_names, 1):\n",
        "    df = spark.read.csv(file_name, header=True)  # Assuming CSV files have a header\n",
        "    # Select the required columns\n",
        "    df_selected = df.select(selected_columns)\n",
        "    # Create the DataFrame name as df1, df2, df3, etc.\n",
        "    df_name = f\"df{i}\"\n",
        "    # Assign the DataFrame to a dictionary with key as DataFrame name\n",
        "    dataframes[df_name] = df_selected\n",
        "\n",
        "# Access the DataFrames using keys like dataframes[\"df1\"], dataframes[\"df2\"], etc.\n"
      ],
      "metadata": {
        "id": "ZQ7xCy_jvwI_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access individual DataFrames using keys\n",
        "df23 = dataframes[\"df23\"]\n",
        "# and so on...\n",
        "\n",
        "# Show the contents of individual DataFrames\n",
        "df23.show()\n",
        "# and so on...\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79D0qCA5zU2I",
        "outputId": "1838cbc6-b6cd-4396-999f-7aa1da77a16d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------------+\n",
            "|    Id|                Name|\n",
            "+------+--------------------+\n",
            "|900000| Cabinet 03: Weather|\n",
            "|900001|Workouts in a Bin...|\n",
            "|900002|Las 120 jornadas ...|\n",
            "|900004|The Traveler: An ...|\n",
            "|900006|      Vertical Smile|\n",
            "|900009| The Best Laid Plans|\n",
            "|900012|Morning, Noon & N...|\n",
            "|900013|Sex, Botany And E...|\n",
            "|900015|              Memory|\n",
            "|900017|Pandora's Breeche...|\n",
            "|900018|Scientists Anonym...|\n",
            "|900022|All Under Heaven:...|\n",
            "|900024|Confessor (Herbie...|\n",
            "|900025|           Confessor|\n",
            "|900029|      The Naked Face|\n",
            "|900030|Are You Afraid Of...|\n",
            "|900036|The World's Stupi...|\n",
            "|900038|The Secret of the...|\n",
            "|900041|The Book of Stran...|\n",
            "|900043| The House You Build|\n",
            "+------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import reduce\n",
        "\n",
        "# Combine all DataFrames into one DataFrame using union\n",
        "combined_df = reduce(lambda df1, df2: df1.union(df2), dataframes.values())\n",
        "\n",
        "# Show the combined DataFrame\n",
        "combined_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7FRETsAvwL_",
        "outputId": "3d51820e-ef50-4e9c-f61a-7c037057e7b6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+\n",
            "| Id|                Name|\n",
            "+---+--------------------+\n",
            "|  1|Harry Potter and ...|\n",
            "|  2|Harry Potter and ...|\n",
            "|  3|Harry Potter and ...|\n",
            "|  4|Harry Potter and ...|\n",
            "|  5|Harry Potter and ...|\n",
            "|  6|Harry Potter and ...|\n",
            "|  8|Harry Potter Boxe...|\n",
            "|  9|\"Unauthorized Har...|\n",
            "| 10|Harry Potter Coll...|\n",
            "| 12|The Ultimate Hitc...|\n",
            "| 13|The Ultimate Hitc...|\n",
            "| 14|The Hitchhiker's ...|\n",
            "| 18|The Ultimate Hitc...|\n",
            "| 21|A Short History o...|\n",
            "| 22|Bill Bryson's Afr...|\n",
            "| 23|Bryson's Dictiona...|\n",
            "| 24|In a Sunburned Co...|\n",
            "| 25|I'm a Stranger He...|\n",
            "| 26|The Lost Continen...|\n",
            "| 27|Neither Here nor ...|\n",
            "+---+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the number of rows in the combined DataFrame\n",
        "print(\"Number of rows in the combined DataFrame:\", combined_df.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xURLpHp_vwOu",
        "outputId": "b72236fd-f7b1-4a08-f931-d5dd871614bf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in the combined DataFrame: 1886019\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the data types of each column in the combined DataFrame\n",
        "combined_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzlvQozIvwR9",
        "outputId": "104fd056-2bda-4528-9ae3-f7a4c515ad2a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Id: string (nullable = true)\n",
            " |-- Name: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### User ID and their individual Ratings:"
      ],
      "metadata": {
        "id": "LFcq12yJ2HRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the file names for user ratings\n",
        "user_ratings_files = [\n",
        "    \"/content/drive/MyDrive/Data Science Projects/Book Recommendation System/Datasets/user_rating_0_to_1000.csv\",\n",
        "    '/content/drive/MyDrive/Data Science Projects/Book Recommendation System/Datasets/user_rating_1000_to_2000.csv',\n",
        "    '/content/drive/MyDrive/Data Science Projects/Book Recommendation System/Datasets/user_rating_2000_to_3000.csv',\n",
        "    '/content/drive/MyDrive/Data Science Projects/Book Recommendation System/Datasets/user_rating_3000_to_4000.csv',\n",
        "    '/content/drive/MyDrive/Data Science Projects/Book Recommendation System/Datasets/user_rating_5000_to_6000.csv',\n",
        "    '/content/drive/MyDrive/Data Science Projects/Book Recommendation System/Datasets/user_rating_6000_to_11000.csv',\n",
        "    \"/content/drive/MyDrive/Data Science Projects/Book Recommendation System/Datasets/user_rating_4000_to_5000.csv\"\n",
        "    # Add all file names url here\n",
        "]\n",
        "\n",
        "# Load each CSV file containing user ratings into separate DataFrames\n",
        "user_ratings_dataframes = {}\n",
        "for i, file_name in enumerate(user_ratings_files, 1):\n",
        "    df = spark.read.csv(file_name, header=True)  # Assuming CSV files have a header\n",
        "    # Extract the DataFrame name from the file name\n",
        "    df_name = f\"user_ratings_df{i}\"\n",
        "    # Assign the DataFrame to a dictionary with key as DataFrame name\n",
        "    user_ratings_dataframes[df_name] = df\n",
        "\n",
        "# Access the user ratings DataFrames using keys like user_ratings_dataframes[\"user_ratings_df1\"], user_ratings_dataframes[\"user_ratings_df2\"], etc.\n"
      ],
      "metadata": {
        "id": "ucFZXweZvwU5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access individual user ratings DataFrames using keys\n",
        "user_ratings_df1 = user_ratings_dataframes[\"user_ratings_df1\"]\n",
        "user_ratings_df2 = user_ratings_dataframes[\"user_ratings_df2\"]\n",
        "# and so on...\n",
        "\n",
        "# Show the contents of individual user ratings DataFrames\n",
        "user_ratings_df1.show()\n",
        "user_ratings_df2.show()\n",
        "# and so on...\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCTUJ13S3YgM",
        "outputId": "fb0638db-6ebe-46d1-c4c8-884bcef48dd9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+---------------+\n",
            "| ID|                Name|         Rating|\n",
            "+---+--------------------+---------------+\n",
            "|  1|Agile Web Develop...| it was amazing|\n",
            "|  1|The Restaurant at...| it was amazing|\n",
            "|  1|          Siddhartha| it was amazing|\n",
            "|  1|The Clock of the ...|really liked it|\n",
            "|  1|Ready Player One ...|really liked it|\n",
            "|  1|The Hunger Games ...| it was amazing|\n",
            "|  1|The Clue in the E...| it was amazing|\n",
            "|  1|The Authoritative...| it was amazing|\n",
            "|  1|The Clue of the B...| it was amazing|\n",
            "|  1|The Clue of the H...| it was amazing|\n",
            "|  1|The Clue of the S...| it was amazing|\n",
            "|  1|The Return of the...| it was amazing|\n",
            "|  1|The Name of the Rose|       liked it|\n",
            "|  1|Blue Mars (Mars T...|       liked it|\n",
            "|  1|Give and Take: A ...| it was amazing|\n",
            "|  1|Mindset: The New ...|really liked it|\n",
            "|  1|Bad Blood: Secret...|really liked it|\n",
            "|  1|Dark Apprentice (...|       liked it|\n",
            "|  1|A Short History o...| it was amazing|\n",
            "|  1|The Mystery of th...| it was amazing|\n",
            "+---+--------------------+---------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+----+------+--------------------+\n",
            "|  ID|  Name|              Rating|\n",
            "+----+------+--------------------+\n",
            "|1000|Rating|This user doesn't...|\n",
            "|1001|Rating|This user doesn't...|\n",
            "|1002|Rating|This user doesn't...|\n",
            "|1003|Rating|This user doesn't...|\n",
            "|1005|Rating|This user doesn't...|\n",
            "|1010|Rating|This user doesn't...|\n",
            "|1011|Rating|This user doesn't...|\n",
            "|1012|Rating|This user doesn't...|\n",
            "|1019|Rating|This user doesn't...|\n",
            "|1022|Rating|This user doesn't...|\n",
            "|1027|Rating|This user doesn't...|\n",
            "|1028|Rating|This user doesn't...|\n",
            "|1031|Rating|This user doesn't...|\n",
            "|1032|Rating|This user doesn't...|\n",
            "|1033|Rating|This user doesn't...|\n",
            "|1034|Rating|This user doesn't...|\n",
            "|1035|Rating|This user doesn't...|\n",
            "|1036|Rating|This user doesn't...|\n",
            "|1038|Rating|This user doesn't...|\n",
            "|1039|Rating|This user doesn't...|\n",
            "+----+------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all user ratings DataFrames into one DataFrame using union\n",
        "combined_user_ratings_df = reduce(lambda df1, df2: df1.union(df2), user_ratings_dataframes.values())\n",
        "\n",
        "# Show the combined DataFrame\n",
        "combined_user_ratings_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NM8nLtrvwYA",
        "outputId": "2005bdfc-c00d-440d-de62-80e672dc99b9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+---------------+\n",
            "| ID|                Name|         Rating|\n",
            "+---+--------------------+---------------+\n",
            "|  1|Agile Web Develop...| it was amazing|\n",
            "|  1|The Restaurant at...| it was amazing|\n",
            "|  1|          Siddhartha| it was amazing|\n",
            "|  1|The Clock of the ...|really liked it|\n",
            "|  1|Ready Player One ...|really liked it|\n",
            "|  1|The Hunger Games ...| it was amazing|\n",
            "|  1|The Clue in the E...| it was amazing|\n",
            "|  1|The Authoritative...| it was amazing|\n",
            "|  1|The Clue of the B...| it was amazing|\n",
            "|  1|The Clue of the H...| it was amazing|\n",
            "|  1|The Clue of the S...| it was amazing|\n",
            "|  1|The Return of the...| it was amazing|\n",
            "|  1|The Name of the Rose|       liked it|\n",
            "|  1|Blue Mars (Mars T...|       liked it|\n",
            "|  1|Give and Take: A ...| it was amazing|\n",
            "|  1|Mindset: The New ...|really liked it|\n",
            "|  1|Bad Blood: Secret...|really liked it|\n",
            "|  1|Dark Apprentice (...|       liked it|\n",
            "|  1|A Short History o...| it was amazing|\n",
            "|  1|The Mystery of th...| it was amazing|\n",
            "+---+--------------------+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the number of rows in the combined user ratings DataFrame\n",
        "print(\"Number of rows in the combined user ratings DataFrame:\", combined_user_ratings_df.count())\n",
        "\n",
        "# Show the data types of each column in the combined user ratings DataFrame\n",
        "combined_user_ratings_df.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YYnBmIN4poa",
        "outputId": "52ad8c59-9a65-4ee0-fe19-fb1f945fdf1f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in the combined user ratings DataFrame: 362596\n",
            "root\n",
            " |-- ID: string (nullable = true)\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Rating: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing:"
      ],
      "metadata": {
        "id": "VVVMhUbu5ItN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename the 'Id' column to 'Book ID' in the combined DataFrame of books\n",
        "combined_df = combined_df.withColumnRenamed(\"Id\", \"Book ID\")\n",
        "# Rename the 'ID' column to 'User ID' in the combined DataFrame of user ratings\n",
        "combined_user_ratings_df = combined_user_ratings_df.withColumnRenamed(\"ID\", \"User ID\")\n"
      ],
      "metadata": {
        "id": "sS0oq2jB4plH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the occurrences of each unique value in the 'Rating' column\n",
        "rating_counts = combined_user_ratings_df.groupBy(\"Rating\").count()\n",
        "\n",
        "# Show the unique values and their counts in the 'Rating' column\n",
        "rating_counts.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufw5-nv6GfrG",
        "outputId": "c13d77e6-0642-4ec7-d881-0c6e39718534"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------+\n",
            "|              Rating| count|\n",
            "+--------------------+------+\n",
            "|     did not like it|  7806|\n",
            "|     really liked it|132779|\n",
            "| Mr. Feynman!\"\": ...|     1|\n",
            "|            Proverbs|     1|\n",
            "|                #1)\"|    35|\n",
            "|            liked it| 96021|\n",
            "| en el país de lo...|     1|\n",
            "|           it was ok| 28806|\n",
            "|          1931-1932\"|    19|\n",
            "|      it was amazing| 92313|\n",
            "|This user doesn't...|  4765|\n",
            "| Why We Say the T...|     1|\n",
            "|     \"\" the Vibrator|     1|\n",
            "| But\"\" Thinking a...|     2|\n",
            "|                #2)\"|     2|\n",
            "| Vol. 2: A Seth B...|     1|\n",
            "|                #3)\"|     3|\n",
            "|    African American|     6|\n",
            "|               #12)\"|     1|\n",
            "|                #5)\"|     1|\n",
            "+--------------------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*** Note: *** Here, it looks like the ratings given by users are full of inconsistent values. We need to clean these. We are going to replace the texts with reasonable numerical values (on a scale of 1-10, where, 1=did not like, 10=really liked and 0 = did not read):\n",
        "\n",
        "really liked it = 10\n",
        "\n",
        "did not like it = 1\n",
        "\n",
        " #6)\" = 6\n",
        "\n",
        " #3)\" = 3\n",
        "\n",
        "liked it = 8\n",
        " #7)\" = 7\n",
        "\n",
        "it was ok = 5\n",
        "\n",
        "it was amazing = 9\n",
        "\n",
        " #4)\" = 4\n",
        "\n",
        " #1)\" = 1\n",
        "\n",
        "\n",
        " #2)\" = 2\n",
        "\n",
        " #5)\" = 5\n",
        "\n",
        " Anything else will be considered = 5 (average value)"
      ],
      "metadata": {
        "id": "NRl8fNh4GHe2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when\n",
        "\n",
        "# Define the mapping from text ratings to numerical values\n",
        "rating_mapping = { \"really liked it\": 10,\n",
        "    \"did not like it\": 1,\n",
        "    \"#6\\\"\": 6,\n",
        "    \"#3\\\"\": 3,\n",
        "    \"liked it\": 8,\n",
        "    \"#7\\\"\": 7,\n",
        "    \"it was ok\": 5,\n",
        "    \"it was amazing\": 9,\n",
        "    \"#4\\\"\": 4,\n",
        "    \"#1\\\"\": 1,\n",
        "    \"#2\\\"\": 2,\n",
        "    \"#5\\\"\": 5\n",
        "}\n",
        "\n",
        "# Define the default numerical value for anything else\n",
        "default_rating = 0  # Considered as unknown or null\n",
        "\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# Define a user-defined function to apply the mapping to each value in the 'Rating' column\n",
        "def map_rating(rating):\n",
        "    if rating in rating_mapping:\n",
        "        return rating_mapping[rating]\n",
        "    else:\n",
        "        return default_rating\n",
        "\n",
        "# Register the user-defined function\n",
        "map_rating_udf = udf(map_rating, IntegerType())\n",
        "\n",
        "# Apply the user-defined function to the 'Rating' column\n",
        "cleaned_ratings_df = combined_user_ratings_df.withColumn(\"Rating\", map_rating_udf(combined_user_ratings_df[\"Rating\"]))\n",
        "\n",
        "# Show the updated DataFrame\n",
        "cleaned_ratings_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCWDIjKZGJ-C",
        "outputId": "a730b955-759c-4a93-9f1c-68f28757f792"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+------+\n",
            "|User ID|                Name|Rating|\n",
            "+-------+--------------------+------+\n",
            "|      1|Agile Web Develop...|     9|\n",
            "|      1|The Restaurant at...|     9|\n",
            "|      1|          Siddhartha|     9|\n",
            "|      1|The Clock of the ...|    10|\n",
            "|      1|Ready Player One ...|    10|\n",
            "|      1|The Hunger Games ...|     9|\n",
            "|      1|The Clue in the E...|     9|\n",
            "|      1|The Authoritative...|     9|\n",
            "|      1|The Clue of the B...|     9|\n",
            "|      1|The Clue of the H...|     9|\n",
            "|      1|The Clue of the S...|     9|\n",
            "|      1|The Return of the...|     9|\n",
            "|      1|The Name of the Rose|     8|\n",
            "|      1|Blue Mars (Mars T...|     8|\n",
            "|      1|Give and Take: A ...|     9|\n",
            "|      1|Mindset: The New ...|    10|\n",
            "|      1|Bad Blood: Secret...|    10|\n",
            "|      1|Dark Apprentice (...|     8|\n",
            "|      1|A Short History o...|     9|\n",
            "|      1|The Mystery of th...|     9|\n",
            "+-------+--------------------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the occurrences of each unique value in the 'Rating' column\n",
        "rating_counts = cleaned_ratings_df.groupBy(\"Rating\").count()\n",
        "\n",
        "# Show the unique values and their counts in the 'Rating' column\n",
        "rating_counts.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8xZK_JUL51X",
        "outputId": "ddc54461-1b6a-41fb-bee1-9fc452ad66a2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+\n",
            "|Rating| count|\n",
            "+------+------+\n",
            "|     1|  7806|\n",
            "|     5| 28806|\n",
            "|     9| 92313|\n",
            "|     8| 96021|\n",
            "|    10|132779|\n",
            "|     0|  4871|\n",
            "+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Joining the two dataframes to create a final dataframe:"
      ],
      "metadata": {
        "id": "ss957Cec6GPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Perform an inner join on the 'Name' column\n",
        "final_df = cleaned_ratings_df.join(combined_df, combined_df[\"Name\"] == combined_user_ratings_df[\"Name\"], \"left\").drop(combined_df[\"Name\"])\n",
        "\n",
        "# Show the final DataFrame\n",
        "final_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vNGtgyS4pib",
        "outputId": "2004a587-afea-4d1e-9a98-7de691614810"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+------+-------+\n",
            "|User ID|                Name|Rating|Book ID|\n",
            "+-------+--------------------+------+-------+\n",
            "|   8693|28 Barbary Lane: ...|     5|  16264|\n",
            "|      1|Agile Web Develop...|     9|     45|\n",
            "|   8698|Angry White Pyjam...|     8| 198051|\n",
            "|      1|Bad Blood: Secret...|    10|   NULL|\n",
            "|      1|Blue Mars (Mars T...|     8|  41131|\n",
            "|   8693|Children of God (...|     8|  16948|\n",
            "|   9164|Children of God (...|     8|  16948|\n",
            "|   6675|Dahlia Season: St...|    10|1243755|\n",
            "|      1|Dark Apprentice (...|     8|1146551|\n",
            "|      1|Dark Apprentice (...|     8|2671637|\n",
            "|   8702|Doing Harm: The T...|     9|   NULL|\n",
            "|      1|Give and Take: A ...|     9|   NULL|\n",
            "|   8706|Hell Is a Very Sm...|     9|   NULL|\n",
            "|   8706|  Here Come The Dogs|     9|   NULL|\n",
            "|      1|Mindset: The New ...|    10|   NULL|\n",
            "|   6675|No One Belongs He...|    10| 113429|\n",
            "|   6675|No One Belongs He...|    10|1561322|\n",
            "|   6675|No One Belongs He...|    10|2066751|\n",
            "|   6765|No One Belongs He...|    10| 113429|\n",
            "|   6765|No One Belongs He...|    10|1561322|\n",
            "+-------+--------------------+------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of rows in the final DataFrame\n",
        "row_count = final_df.count()\n",
        "\n",
        "# Print the number of rows\n",
        "print(\"Number of rows in the final DataFrame:\", row_count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFq0j1ID4paO",
        "outputId": "4673d4a3-2eba-49ec-c6b6-c5a40755e282"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in the final DataFrame: 1184215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking and Handling Missing Values :"
      ],
      "metadata": {
        "id": "fLmHutosRbPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Define a dictionary to store the count of null values for each column\n",
        "null_counts = {}\n",
        "\n",
        "# Iterate over each column and count the number of null values\n",
        "\n",
        "for col_name in final_df.columns:\n",
        "    # Count the number of null values in the column\n",
        "    null_count = final_df.filter(col(col_name).isNull()).count()\n",
        "    # Store the null count in the dictionary\n",
        "    null_counts[col_name] = null_count\n",
        "\n",
        "# Print the count of null values for each column\n",
        "for col_name, count in null_counts.items():\n",
        "    print(f\"Column '{col_name}': {count} null values\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-0DBdaQ4pXh",
        "outputId": "a10e7cfb-375b-4a3a-bd12-969f5cd56611"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column 'User ID': 0 null values\n",
            "Column 'Name': 0 null values\n",
            "Column 'Rating': 0 null values\n",
            "Column 'Book ID': 130992 null values\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*** Note***: Here, there are 130992 null values in Book ID. I will Try to create unique random values for these missing Book IDs.\n",
        "I used the following code snippet:\n",
        "\n",
        "```python\n",
        "from pyspark.sql.functions import col, lit\n",
        "\n",
        "# Step 1: Create a new DataFrame with only \"Name\" and \"Book ID\" columns\n",
        "name_book_id_df = final_df.select(\"Name\", \"Book ID\")\n",
        "\n",
        "# Step 2: Filter out rows where \"Book ID\" is not null\n",
        "filtered_df = name_book_id_df.filter(col(\"Book ID\").isNotNull())\n",
        "\n",
        "# Step 3: Find the max Book ID\n",
        "max_book_id_row = filtered_df.agg({\"Book ID\": \"max\"}).first()\n",
        "max_book_id = max_book_id_row[0] if max_book_id_row[0] else 0  # Get the max Book ID or 0 if it's null\n",
        "\n",
        "# Step 4: Iterate over unique \"Name\" values and assign unique Book ID to each missing value\n",
        "unique_names = filtered_df.select(\"Name\").distinct().collect()\n",
        "for name_row in unique_names:\n",
        "    name = name_row[\"Name\"]\n",
        "    if not name_book_id_df.filter((col(\"Name\") == name) & (col(\"Book ID\").isNotNull())).count():\n",
        "        # Assign a unique Book ID to the name\n",
        "        max_book_id += 1\n",
        "        name_book_id_df = name_book_id_df.withColumn(\"Book ID\",\n",
        "                                                     when(col(\"Name\") == name, lit(max_book_id)).otherwise(col(\"Book ID\")))\n",
        "\n",
        "# Join the original final_df with the modified name_book_id_df to fill in the missing Book IDs\n",
        "final_df = final_df.join(name_book_id_df, \"Name\", \"left\")\n",
        "\n",
        "# Show the updated final DataFrame\n",
        "final_df.show()\n",
        "```\n",
        "\n",
        "*** Note: *** But it seems this is taking too long with my Limited Resources. So i am going to delete these rows with missing data."
      ],
      "metadata": {
        "id": "GEQ07ebwaINQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove rows with null values in the \"Book ID\" column\n",
        "final_df = final_df.na.drop(subset=[\"Book ID\"])\n"
      ],
      "metadata": {
        "id": "Bb14MRCWmtXw"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Define a dictionary to store the count of null values for each column\n",
        "null_counts = {}\n",
        "\n",
        "# Iterate over each column and count the number of null values\n",
        "\n",
        "for col_name in final_df.columns:\n",
        "    # Count the number of null values in the column\n",
        "    null_count = final_df.filter(col(col_name).isNull()).count()\n",
        "    # Store the null count in the dictionary\n",
        "    null_counts[col_name] = null_count\n",
        "\n",
        "# Print the count of null values for each column\n",
        "for col_name, count in null_counts.items():\n",
        "    print(f\"Column '{col_name}': {count} null values\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0QveaknfZ7S",
        "outputId": "2bff5167-0c51-408f-e2ca-bdc9cb1216b5"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column 'User ID': 0 null values\n",
            "Column 'Name': 0 null values\n",
            "Column 'Rating': 0 null values\n",
            "Column 'Book ID': 0 null values\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the Collaborative Filtering Recommendation Model:"
      ],
      "metadata": {
        "id": "gN0TWz_xSAN_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Alternating Least Squares (ALS) algorithm is a collaborative filtering algorithm used for recommendation systems. It is particularly useful for large-scale collaborative filtering problems, especially when dealing with sparse datasets. Here's an explanation of how ALS works:\n",
        "\n",
        "1. **Matrix Factorization**: ALS is based on the idea of matrix factorization. In a recommendation system context, we represent the user-item interaction matrix as a sparse matrix where rows correspond to users and columns correspond to items (e.g., movies, products). The goal is to decompose this matrix into two lower-rank matrices: one that represents users and their latent features, and the other represents items and their latent features.\n",
        "\n",
        "2. **Objective Function**: ALS aims to minimize the difference between the actual ratings given by users and the predicted ratings obtained from the matrix factorization. This is typically achieved by minimizing the sum of squared errors (hence, \"Least Squares\"). The objective function is optimized iteratively using alternating least squares, where one set of variables (either user or item latent features) is fixed while the other set is optimized.\n",
        "\n",
        "3. **Alternating Least Squares (ALS)**: The ALS algorithm alternates between two steps:\n",
        "   - **Fixing User Latent Features**: Given the current item latent features, the user latent features are updated by solving a least squares problem. This step aims to find the optimal representation of each user in the latent feature space.\n",
        "   - **Fixing Item Latent Features**: Given the updated user latent features, the item latent features are updated by solving another least squares problem. This step aims to find the optimal representation of each item in the latent feature space.\n",
        "   These steps are repeated iteratively until convergence or until a stopping criterion is met.\n",
        "\n",
        "4. **Cold Start Handling**: ALS includes a \"cold start\" strategy to handle new users or items for which no rating data is available. This strategy typically involves either dropping or providing default values for such users/items during training or prediction.\n",
        "\n",
        "5. **Regularization**: ALS often incorporates regularization techniques to prevent overfitting during training. Regularization helps to generalize the learned latent features and avoid fitting the noise in the data too closely.\n",
        "\n",
        "6. **Scalability**: ALS is highly parallelizable and scalable, making it suitable for large-scale recommendation systems. It can be efficiently implemented in distributed computing frameworks like Apache Spark.\n",
        "\n",
        "Overall, ALS is a popular and effective algorithm for collaborative filtering-based recommendation systems, offering good performance and scalability for large datasets."
      ],
      "metadata": {
        "id": "CmeYvO_VqlZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.recommendation import ALS\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "(train_data, test_data) = final_df.randomSplit([0.8, 0.2], seed=123)\n",
        "\n",
        "# Define StringIndexer for User ID column\n",
        "user_indexer = StringIndexer(inputCol=\"User ID\", outputCol=\"user_index\",handleInvalid=\"keep\")\n",
        "\n",
        "# Fit StringIndexer to training data\n",
        "user_indexer_model = user_indexer.fit(train_data)\n",
        "\n",
        "# Transform training data to add user_index column\n",
        "train_data_indexed = user_indexer_model.transform(train_data)\n",
        "\n",
        "# Transform test data using the fitted StringIndexer for User ID\n",
        "test_data_indexed = user_indexer_model.transform(test_data)\n",
        "\n",
        "# Define StringIndexer for Book ID column\n",
        "book_indexer = StringIndexer(inputCol=\"Book ID\", outputCol=\"book_index\", handleInvalid=\"keep\")\n",
        "\n",
        "# Fit StringIndexer to training data\n",
        "book_indexer_model = book_indexer.fit(train_data_indexed)\n",
        "\n",
        "# Transform training data to add book_index column\n",
        "train_data_indexed = book_indexer_model.transform(train_data_indexed)\n",
        "\n",
        "# Transform test data using the fitted StringIndexer for Book ID\n",
        "test_data_indexed = book_indexer_model.transform(test_data_indexed)\n",
        "\n",
        "# Define ALS model using the indexed user and book columns\n",
        "als = ALS(userCol=\"user_index\", itemCol=\"book_index\", ratingCol=\"Rating\", coldStartStrategy=\"drop\")\n",
        "\n",
        "# Train ALS model on training data\n",
        "model = als.fit(train_data_indexed)\n",
        "\n",
        "# Generate predictions on test data\n",
        "predictions = model.transform(test_data_indexed)\n",
        "\n",
        "# Evaluate the model using Root Mean Squared Error (RMSE)\n",
        "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"Rating\", predictionCol=\"prediction\")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1t5OvQt4pVI",
        "outputId": "aa2e47bd-1eaf-4d7f-ae03-2d3f38d2baf3"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Squared Error (RMSE): 1.5533601663969065\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Looks like our model performs superbly well with a very low Root Mean Square Value."
      ],
      "metadata": {
        "id": "CreNeK7NyWoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate top 'n' recommendations for each user\n",
        "# let's find top 10 recommendations for all the users\n",
        "user_recommendations = model.recommendForAllUsers(10)\n",
        "user_recommendations.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sV29dC54pTE",
        "outputId": "84d3cf46-aea9-4b61-cf74-1bd87f6a0916"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|user_index|recommendations                                                                                                                                                                                                 |\n",
            "+----------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|1         |[{22892, 13.214581}, {20493, 12.391882}, {29573, 11.795004}, {91615, 11.760404}, {90143, 11.760404}, {89438, 11.760404}, {78561, 11.760404}, {59585, 11.760404}, {53870, 11.760404}, {51413, 11.760404}]        |\n",
            "|3         |[{15902, 11.077564}, {22489, 10.94491}, {30292, 10.8600445}, {27399, 10.8600445}, {12530, 10.822273}, {12507, 10.77191}, {12118, 10.754933}, {8108, 10.733342}, {15162, 10.691326}, {31680, 10.584063}]         |\n",
            "|5         |[{30777, 13.117385}, {15902, 12.997025}, {28623, 12.699356}, {12530, 12.630333}, {38685, 12.604985}, {14803, 12.281872}, {34605, 12.177716}, {14502, 12.103762}, {8108, 12.0879}, {31680, 12.048383}]           |\n",
            "|6         |[{12530, 11.782419}, {91615, 11.742324}, {90143, 11.742324}, {89438, 11.742324}, {78561, 11.742324}, {59585, 11.742324}, {53870, 11.742324}, {51413, 11.742324}, {50619, 11.742324}, {36691, 11.326018}]        |\n",
            "|9         |[{34605, 13.988733}, {32168, 12.062561}, {16678, 11.839497}, {14428, 11.760111}, {21461, 11.697928}, {80346, 11.627745}, {78514, 11.627745}, {78512, 11.627745}, {78509, 11.627745}, {78507, 11.627745}]        |\n",
            "|12        |[{52153, 12.40854}, {52150, 12.40854}, {91615, 12.207707}, {90143, 12.207707}, {89438, 12.207707}, {78561, 12.207707}, {59585, 12.207707}, {53870, 12.207707}, {51413, 12.207707}, {50619, 12.207707}]          |\n",
            "|13        |[{18011, 11.823896}, {16678, 11.606156}, {14581, 11.573533}, {10604, 11.5619955}, {13588, 11.5332155}, {34605, 11.417496}, {14237, 11.41354}, {9076, 11.364479}, {15902, 11.360917}, {19059, 11.345205}]        |\n",
            "|15        |[{34605, 13.489834}, {16678, 13.291103}, {12799, 12.485604}, {12401, 12.485604}, {19962, 12.408546}, {9157, 12.263747}, {71358, 12.241167}, {57478, 12.241167}, {50081, 12.241167}, {11941, 12.234127}]         |\n",
            "|16        |[{22892, 12.036988}, {91615, 11.829725}, {90143, 11.829725}, {89438, 11.829725}, {78561, 11.829725}, {59585, 11.829725}, {53870, 11.829725}, {51413, 11.829725}, {50619, 11.829725}, {34605, 11.702737}]        |\n",
            "|17        |[{31304, 12.600515}, {28220, 12.600515}, {24199, 12.580082}, {91782, 12.5174055}, {91781, 12.5174055}, {29560, 12.333252}, {14559, 12.113936}, {24255, 12.11381}, {24005, 12.018084}, {13935, 11.986983}]       |\n",
            "|19        |[{18583, 14.894133}, {33090, 14.743578}, {33664, 14.557185}, {20947, 14.345608}, {29573, 13.903914}, {28863, 13.812912}, {11331, 13.70164}, {26545, 13.656455}, {25093, 13.656455}, {10573, 13.634874}]         |\n",
            "|20        |[{26462, 11.469659}, {20155, 11.26566}, {91615, 11.265572}, {90143, 11.265572}, {89438, 11.265572}, {78561, 11.265572}, {59585, 11.265572}, {53870, 11.265572}, {51413, 11.265572}, {50619, 11.265572}]         |\n",
            "|22        |[{34605, 12.83705}, {31145, 12.570871}, {22892, 12.4020605}, {80346, 12.203682}, {78514, 12.203682}, {78512, 12.203682}, {78509, 12.203682}, {78507, 12.203682}, {78505, 12.203682}, {91615, 12.199448}]        |\n",
            "|26        |[{34605, 12.682865}, {91615, 12.633258}, {90143, 12.633258}, {89438, 12.633258}, {78561, 12.633258}, {59585, 12.633258}, {53870, 12.633258}, {51413, 12.633258}, {50619, 12.633258}, {80346, 12.619296}]        |\n",
            "|27        |[{34605, 12.005559}, {91615, 11.47261}, {90143, 11.47261}, {89438, 11.47261}, {78561, 11.47261}, {59585, 11.47261}, {53870, 11.47261}, {51413, 11.47261}, {50619, 11.47261}, {22892, 11.459322}]                |\n",
            "|28        |[{34605, 14.827658}, {16678, 12.245492}, {91615, 11.8900795}, {90143, 11.8900795}, {89438, 11.8900795}, {78561, 11.8900795}, {59585, 11.8900795}, {53870, 11.8900795}, {51413, 11.8900795}, {50619, 11.8900795}]|\n",
            "|31        |[{91615, 12.355283}, {90143, 12.355283}, {89438, 12.355283}, {78561, 12.355283}, {59585, 12.355283}, {53870, 12.355283}, {51413, 12.355283}, {50619, 12.355283}, {79920, 11.20422}, {63793, 11.20422}]          |\n",
            "|34        |[{22156, 12.591749}, {25976, 12.485099}, {24483, 12.485099}, {23038, 12.359663}, {30334, 12.306415}, {30255, 12.306415}, {26956, 12.306415}, {26895, 12.306415}, {17001, 12.139993}, {33633, 11.992703}]        |\n",
            "|35        |[{34605, 12.163206}, {91615, 11.893913}, {90143, 11.893913}, {89438, 11.893913}, {78561, 11.893913}, {59585, 11.893913}, {53870, 11.893913}, {51413, 11.893913}, {50619, 11.893913}, {52153, 11.740928}]        |\n",
            "|37        |[{31167, 12.418376}, {91615, 11.929238}, {90143, 11.929238}, {89438, 11.929238}, {78561, 11.929238}, {59585, 11.929238}, {53870, 11.929238}, {51413, 11.929238}, {50619, 11.929238}, {22892, 11.908589}]        |\n",
            "+----------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seeing Recommendations for a specific User ID:\n",
        "\n",
        "We Want to Recommend Top 10 Books to Specific Users that The user is predicted to Give a rating greater than or equal to 9."
      ],
      "metadata": {
        "id": "mC4WCV6ewq_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "specific_user_index = 37  # Replace 123 with the desired user_index\n",
        "\n",
        "# Filter the DataFrame to get recommendations for the specific user_index\n",
        "specific_user_recommendations = user_recommendations.filter(user_recommendations.user_index == specific_user_index)\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "# Apply an additional filter to show recommendations with ratings greater than or equal to 9\n",
        "specific_user_recommendations = specific_user_recommendations.withColumn(\"recommendations\",\n",
        "                                                                          F.expr(\"filter(recommendations, x -> x.rating >= 9)\"))\n",
        "\n",
        "# Show the recommendations for the specific user_index\n",
        "specific_user_recommendations.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYaDI60YvD5m",
        "outputId": "e19ba28e-f8f9-4ce6-968d-f3b2344d3bc4"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|user_index|recommendations                                                                                                                                                                                         |\n",
            "+----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|37        |[{31167, 12.418376}, {91615, 11.929238}, {90143, 11.929238}, {89438, 11.929238}, {78561, 11.929238}, {59585, 11.929238}, {53870, 11.929238}, {51413, 11.929238}, {50619, 11.929238}, {22892, 11.908589}]|\n",
            "+----------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}